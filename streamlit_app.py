import streamlit as st
import requests
from bs4 import BeautifulSoup
import datetime
import re
import json


# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢",
    page_icon="ğŸ“°",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
    <style>
    .main { max-width: 1200px; }
    </style>
""", unsafe_allow_html=True)


# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰èªè¨¼æ©Ÿèƒ½
def check_password():
    """ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›ç¢ºèª"""
    if "password_correct" not in st.session_state:
        st.session_state.password_correct = False
    
    if st.session_state.password_correct:
        return True
    
    # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›ãƒšãƒ¼ã‚¸
    st.title("ğŸ” ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™")
    st.markdown("---")
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.subheader("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        password = st.text_input(
            "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰",
            type="password",
            placeholder="æ•°å­—ã§å…¥åŠ›ã—ã¦ãã ã•ã„"
        )
        
        if st.button("å…¥åŠ›", use_container_width=True):
            if password == "0708":
                st.session_state.password_correct = True
                st.success("âœ… ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‰¿èªã•ã‚Œã¾ã—ãŸï¼")
                st.rerun()
            else:
                st.error("âŒ ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé–“é•ã£ã¦ã„ã¾ã™ã€‚")
    
    st.stop()


# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ç¢ºèª
check_password()


def load_keywords():
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ¤œç´¢èªã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚"""
    try:
        with open('keywords.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get("search_keyword", [])
    except:
        return ['ä¹ƒæœ¨å‚', 'æ«»å‚', 'æ—¥å‘å‚', 'AKB', 'HKT']

def format_date_japanese(date):
    """æ—¥ä»˜ã‚’M/Då½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚"""
    return f"{date.month}/{date.day}"

def get_date_range(days_ago):
    """æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ç¯„å›²ã‚’è¿”ã—ã¾ã™ã€‚"""
    if days_ago == 'all':
        return [datetime.date.today() - datetime.timedelta(days=i) for i in range(7)]
    else:
        return [datetime.date.today() - datetime.timedelta(days=int(days_ago))]

def scrape_yahoo_news(keyword, days_ago='0'):
    """Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¸ãƒ£ãƒ‘ãƒ³ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢ã—ã¾ã™ã€‚"""
    url = f"https://news.yahoo.co.jp/search?p={keyword}&rkf=2&ei=UTF-8"
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    }
    
    found_articles = []
    target_dates = get_date_range(days_ago)
    target_date_strs = [format_date_japanese(d) for d in target_dates]
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        articles = soup.find_all('li', class_='sc-1u4589e-0')
        
        for article in articles:
            date_element = article.find('time')
            if not date_element:
                continue

            date_text = date_element.text
            match = re.search(r'(\d{1,2}/\d{1,2})', date_text)

            if match:
                if days_ago == '0':
                    if match.group(1) not in target_date_strs:
                        continue
                
                title_element = article.find('div', class_='sc-3ls169-0')
                link_element = article.find('a')
                media_element = article.find('span')

                if title_element and link_element:
                    found_articles.append({
                        'title': title_element.text.strip(),
                        'link': link_element.get('href', '#'),
                        'media': media_element.text.strip() if media_element else 'N/A',
                        'publish_time': date_text.strip(),
                        'source': 'Yahoo News'
                    })

    except Exception as e:
        st.error(f"Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
    
    found_articles.sort(key=lambda x: x['publish_time'], reverse=True)
    return found_articles

def scrape_prtimes(keyword, days_ago='0'):
    """PR Timesã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢ã—ã¾ã™ã€‚"""
    search_keyword = keyword.replace(' ', '+')
    url = f"https://prtimes.jp/main/action.php?run=html&page=searchkey&search_word={search_keyword}"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    }
    
    found_articles = []
    target_dates = get_date_range(days_ago)
    target_date_strs = [format_date_japanese(d) for d in target_dates]
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        article_links = soup.find_all('a', class_='release-card_link__ssvnv')
        
        for link_elem in article_links:
            try:
                title_elem = link_elem.find('h3', class_='release-card_title__WLzWi')
                title = title_elem.text.strip() if title_elem else ""
                link = link_elem.get('href', '#')
                
                time_elem = link_elem.find('time')
                time_text = time_elem.text.strip() if time_elem else ""
                
                date_text = ""
                
                if 'å¹´' in time_text:
                    match = re.search(r'(\d+)å¹´(\d+)æœˆ(\d+)æ—¥', time_text)
                    if match:
                        year, month, day = int(match.group(1)), int(match.group(2)), int(match.group(3))
                        try:
                            date_obj = datetime.date(year, month, day)
                            date_text = format_date_japanese(date_obj)
                        except:
                            pass
                elif 'æ™‚é–“å‰' in time_text:
                    date_text = format_date_japanese(datetime.date.today())
                elif 'æ—¥å‰' in time_text:
                    match = re.search(r'(\d+)æ—¥å‰', time_text)
                    if match:
                        days_before = int(match.group(1))
                        date_text = format_date_japanese(datetime.date.today() - datetime.timedelta(days=days_before))
                else:
                    date_text = format_date_japanese(datetime.date.today())
                
                if not date_text or date_text not in target_date_strs:
                    continue
                
                article = link_elem.parent
                while article and article.name != 'article':
                    article = article.parent
                
                company_link = article.find('a', class_='release-card_companyLink__jRgSJ') if article else None
                company = company_link.text.strip() if company_link else 'PR Times'
                
                found_articles.append({
                    'title': title,
                    'link': link if link.startswith('http') else f"https://prtimes.jp{link}",
                    'media': company,
                    'publish_time': time_text,
                    'source': 'PR Times'
                })
            except:
                continue
    
    except Exception as e:
        st.error(f"PR Timesæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
    
    found_articles.sort(key=lambda x: x['publish_time'], reverse=True)
    return found_articles

def display_articles(articles):
    """è¨˜äº‹ãƒªã‚¹ãƒˆã‚’è¡¨ç¤ºã—ã¾ã™ã€‚"""
    if not articles:
        st.info("æ¤œç´¢ã•ã‚ŒãŸè¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    for article in articles:
        with st.container():
            col1, col2 = st.columns([4, 1])
            
            with col1:
                st.markdown(f"[**{article['title']}**]({article['link']})")
                col_a, col_b = st.columns(2)
                with col_a:
                    st.caption(f"ğŸ“º {article['media']}")
                with col_b:
                    st.caption(f"ğŸ• {article['publish_time']}")
            
            with col2:
                badge_color = "#FF6B6B" if article['source'] == "Yahoo News" else "#4ECDC4"
                st.markdown(f"<span style='background-color:{badge_color}; color:white; padding:5px 10px; border-radius:5px; font-size:0.8em;'>{article['source']}</span>", unsafe_allow_html=True)
            
            st.divider()


# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒª
st.title("ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢")
st.markdown("Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹ & PR Timesã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¤œç´¢")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
with st.sidebar:
    st.header("âš™ï¸ æ¤œç´¢è¨­å®š")
    
    date_options = [
        ("ä»Šæ—¥", "0"),
        ("æ˜¨æ—¥", "1"),
        ("2æ—¥å‰", "2"),
        ("3æ—¥å‰", "3"),
        ("å…¨ã¦ (7æ—¥é–“)", "all")
    ]
    date_selected = st.selectbox(
        "ğŸ“… æ¤œç´¢æœŸé–“",
        range(len(date_options)),
        format_func=lambda i: date_options[i][0]
    )
    days_ago = date_options[date_selected][1]
    
    source_options = [
        ("ğŸ”€ ä¸¡æ–¹", "both"),
        ("ğŸ”” Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿", "yahoo"),
        ("ğŸ“¢ PR Timesã®ã¿", "prtimes")
    ]
    source_selected = st.selectbox(
        "ğŸ“¡ æ¤œç´¢ã‚½ãƒ¼ã‚¹",
        range(len(source_options)),
        format_func=lambda i: source_options[i][0]
    )
    source = source_options[source_selected][1]

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
tab1, tab2, tab3 = st.tabs(["ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢", "ğŸ“ æ–°ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "ğŸŒ å…¨ä½“æ¤œç´¢"])

with tab1:
    st.subheader("ç™»éŒ²æ¸ˆã¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢")
    keywords = load_keywords()
    
    selected_keyword = st.selectbox("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¸æŠ", keywords, key="keyword_select")
    
    if st.button("ğŸ” æ¤œç´¢", key="search_btn1"):
        with st.spinner("æ¤œç´¢ä¸­..."):
            yahoo_results = [] if source == 'prtimes' else scrape_yahoo_news(selected_keyword, days_ago)
            prtimes_results = [] if source == 'yahoo' else scrape_prtimes(selected_keyword, days_ago)
            all_results = yahoo_results + prtimes_results
        
        st.success(f"'{selected_keyword}' æ¤œç´¢å®Œäº†ï¼")
        st.metric("åˆè¨ˆè¨˜äº‹æ•°", len(all_results), f"Yahoo: {len(yahoo_results)} | PR Times: {len(prtimes_results)}")
        
        if source == 'both' and (yahoo_results or prtimes_results):
            result_tab1, result_tab2, result_tab3 = st.tabs(["ğŸ“„ å…¨ã¦", "ğŸ”” Yahoo", "ğŸ“¢ PR Times"])
            with result_tab1:
                display_articles(all_results)
            with result_tab2:
                display_articles(yahoo_results)
            with result_tab3:
                display_articles(prtimes_results)
        else:
            display_articles(all_results)

with tab2:
    st.subheader("æ–°ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢")
    new_keyword = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›", placeholder="ä¾‹: AKB48, ä¹ƒæœ¨å‚...")
    
    if st.button("ğŸ” æ¤œç´¢", key="search_btn2"):
        if new_keyword:
            with st.spinner("æ¤œç´¢ä¸­..."):
                yahoo_results = [] if source == 'prtimes' else scrape_yahoo_news(new_keyword, days_ago)
                prtimes_results = [] if source == 'yahoo' else scrape_prtimes(new_keyword, days_ago)
                all_results = yahoo_results + prtimes_results
            
            st.success(f"'{new_keyword}' æ¤œç´¢å®Œäº†ï¼")
            st.metric("åˆè¨ˆè¨˜äº‹æ•°", len(all_results), f"Yahoo: {len(yahoo_results)} | PR Times: {len(prtimes_results)}")
            
            if source == 'both' and (yahoo_results or prtimes_results):
                result_tab1, result_tab2, result_tab3 = st.tabs(["ğŸ“„ å…¨ã¦", "ğŸ”” Yahoo", "ğŸ“¢ PR Times"])
                with result_tab1:
                    display_articles(all_results)
                with result_tab2:
                    display_articles(yahoo_results)
                with result_tab3:
                    display_articles(prtimes_results)
            else:
                display_articles(all_results)
        else:
            st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

with tab3:
    st.subheader("å…¨ã¦ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢")
    keywords = load_keywords()
    
    if st.button(f"ğŸ” {len(keywords)}å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¨ã¦æ¤œç´¢", key="search_btn3"):
        with st.spinner("å…¨ä½“æ¤œç´¢ä¸­..."):
            all_keywords_results = {}
            total_count = 0
            
            progress_bar = st.progress(0)
            for idx, keyword in enumerate(keywords):
                yahoo_results = [] if source == 'prtimes' else scrape_yahoo_news(keyword, days_ago)
                prtimes_results = [] if source == 'yahoo' else scrape_prtimes(keyword, days_ago)
                results = yahoo_results + prtimes_results
                
                if results:
                    all_keywords_results[keyword] = results
                    total_count += len(results)
                
                progress_bar.progress((idx + 1) / len(keywords))
        
        st.success(f"å…¨ä½“æ¤œç´¢å®Œäº†ï¼")
        st.metric("åˆè¨ˆè¨˜äº‹æ•°", total_count)
        
        for keyword in keywords:
            if keyword in all_keywords_results:
                with st.expander(f"**{keyword}** ({len(all_keywords_results[keyword])}å€‹)", expanded=True):
                    display_articles(all_keywords_results[keyword])
