import streamlit as st
import requests
from bs4 import BeautifulSoup
import datetime
import re
import json


# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ“° ë‰´ìŠ¤ ê²€ìƒ‰ê¸°",
    page_icon="ğŸ“°",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
    <style>
    .main { max-width: 1200px; }
    </style>
""", unsafe_allow_html=True)


# ì•”í˜¸ ì¸ì¦ ê¸°ëŠ¥
def check_password():
    """ì•”í˜¸ ì…ë ¥ í™•ì¸"""
    if "password_correct" not in st.session_state:
        st.session_state.password_correct = False
    
    if st.session_state.password_correct:
        return True
    
    # ì•”í˜¸ ì…ë ¥ í˜ì´ì§€
    st.title("ğŸ” ì ‘ê·¼ ê¶Œí•œ")
    st.markdown("---")
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.subheader("ì•”í˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
        password = st.text_input(
            "ì•”í˜¸",
            type="password",
            placeholder="ìˆ«ìë¡œ ì…ë ¥í•˜ì„¸ìš”"
        )
        
        if st.button("ì…ë ¥", use_container_width=True):
            if password == "0708":
                st.session_state.password_correct = True
                st.success("âœ… ì ‘ê·¼ ìŠ¹ì¸ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.rerun()
            else:
                st.error("âŒ ì•”í˜¸ê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    st.stop()


# ì•”í˜¸ í™•ì¸
check_password()


def load_keywords():
    """í‚¤ì›Œë“œ íŒŒì¼ì—ì„œ ê²€ìƒ‰ì–´ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    try:
        with open('keywords.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get("search_keyword", [])
    except:
        return ['ä¹ƒæœ¨å‚', 'æ«»å‚', 'æ—¥å‘å‚', 'AKB', 'HKT']


def format_date_japanese(date):
    """ë‚ ì§œë¥¼ M/D í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    return f"{date.month}/{date.day}"


def get_date_range(days_ago):
    """ì§€ì •ëœ ë‚ ì§œ ë²”ìœ„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if days_ago == 'all':
        return [datetime.date.today() - datetime.timedelta(days=i) for i in range(7)]
    else:
        return [datetime.date.today() - datetime.timedelta(days=int(days_ago))]


def scrape_yahoo_news(keyword, days_ago='0'):
    """Yahoo News Japanì—ì„œ í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    url = f"https://news.yahoo.co.jp/search?p={keyword}&rkf=2&ei=UTF-8"
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    }
    
    found_articles = []
    target_dates = get_date_range(days_ago)
    target_date_strs = [format_date_japanese(d) for d in target_dates]
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        articles = soup.find_all('li', class_='sc-1u4589e-0')
        
        for article in articles:
            date_element = article.find('time')
            if not date_element:
                continue

            date_text = date_element.text
            match = re.search(r'(\d{1,2}/\d{1,2})', date_text)

            if match:
                if days_ago == '0':
                    if match.group(1) not in target_date_strs:
                        continue
                
                title_element = article.find('div', class_='sc-3ls169-0')
                link_element = article.find('a')
                media_element = article.find('span')

                if title_element and link_element:
                    found_articles.append({
                        'title': title_element.text.strip(),
                        'link': link_element.get('href', '#'),
                        'media': media_element.text.strip() if media_element else 'N/A',
                        'publish_time': date_text.strip(),
                        'source': 'Yahoo News'
                    })

    except Exception as e:
        st.error(f"Yahoo News ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    
    found_articles.sort(key=lambda x: x['publish_time'], reverse=True)
    return found_articles


def scrape_prtimes(keyword, days_ago='0'):
    """PR Timesì—ì„œ í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    search_keyword = keyword.replace(' ', '+')
    url = f"https://prtimes.jp/main/action.php?run=html&page=searchkey&search_word={search_keyword}"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    }
    
    found_articles = []
    target_dates = get_date_range(days_ago)
    target_date_strs = [format_date_japanese(d) for d in target_dates]
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        article_links = soup.find_all('a', class_='release-card_link__ssvnv')
        
        for link_elem in article_links:
            try:
                title_elem = link_elem.find('h3', class_='release-card_title__WLzWi')
                title = title_elem.text.strip() if title_elem else ""
                link = link_elem.get('href', '#')
                
                time_elem = link_elem.find('time')
                time_text = time_elem.text.strip() if time_elem else ""
                
                date_text = ""
                
                if 'å¹´' in time_text:
                    match = re.search(r'(\d+)å¹´(\d+)æœˆ(\d+)æ—¥', time_text)
                    if match:
                        year, month, day = int(match.group(1)), int(match.group(2)), int(match.group(3))
                        try:
                            date_obj = datetime.date(year, month, day)
                            date_text = format_date_japanese(date_obj)
                        except:
                            pass
                elif 'æ™‚é–“å‰' in time_text:
                    date_text = format_date_japanese(datetime.date.today())
                elif 'æ—¥å‰' in time_text:
                    match = re.search(r'(\d+)æ—¥å‰', time_text)
                    if match:
                        days_before = int(match.group(1))
                        date_text = format_date_japanese(datetime.date.today() - datetime.timedelta(days=days_before))
                else:
                    date_text = format_date_japanese(datetime.date.today())
                
                if not date_text or date_text not in target_date_strs:
                    continue
                
                article = link_elem.parent
                while article and article.name != 'article':
                    article = article.parent
                
                company_link = article.find('a', class_='release-card_companyLink__jRgSJ') if article else None
                company = company_link.text.strip() if company_link else 'PR Times'
                
                found_articles.append({
                    'title': title,
                    'link': link if link.startswith('http') else f"https://prtimes.jp{link}",
                    'media': company,
                    'publish_time': time_text,
                    'source': 'PR Times'
                })
            except:
                continue
    
    except Exception as e:
        st.error(f"PR Times ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    
    found_articles.sort(key=lambda x: x['publish_time'], reverse=True)
    return found_articles


def display_articles(articles):
    """ê¸°ì‚¬ ëª©ë¡ì„ í‘œì‹œí•©ë‹ˆë‹¤."""
    if not articles:
        st.info("ê²€ìƒ‰ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    for article in articles:
        with st.container():
            col1, col2 = st.columns([4, 1])
            
            with col1:
                st.markdown(f"[**{article['title']}**]({article['link']})")
                col_a, col_b = st.columns(2)
                with col_a:
                    st.caption(f"ğŸ“º {article['media']}")
                with col_b:
                    st.caption(f"ğŸ• {article['publish_time']}")
            
            with col2:
                badge_color = "#FF6B6B" if article['source'] == "Yahoo News" else "#4ECDC4"
                st.markdown(f"<span style='background-color:{badge_color}; color:white; padding:5px 10px; border-radius:5px; font-size:0.8em;'>{article['source']}</span>", unsafe_allow_html=True)
            
            st.divider()


# ë©”ì¸ ì•±
st.title("ğŸ“° ë‰´ìŠ¤ ê²€ìƒ‰ê¸°")
st.markdown("Yahoo News & PR Timesì—ì„œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ì„¸ìš”")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ê²€ìƒ‰ ì„¤ì •")
    
    date_options = [
        ("ì˜¤ëŠ˜", "0"),
        ("ì–´ì œ", "1"),
        ("2ì¼ ì „", "2"),
        ("3ì¼ ì „", "3"),
        ("ì „ì²´ (7ì¼)", "all")
    ]
    date_selected = st.selectbox(
        "ğŸ“… ê²€ìƒ‰ ê¸°ê°„",
        range(len(date_options)),
        format_func=lambda i: date_options[i][0]
    )
    days_ago = date_options[date_selected][1]
    
    source_options = [
        ("ğŸ”€ ë‘˜ ë‹¤", "both"),
        ("ğŸ”” Yahoo Newsë§Œ", "yahoo"),
        ("ğŸ“¢ PR Timesë§Œ", "prtimes")
    ]
    source_selected = st.selectbox(
        "ğŸ“¡ ê²€ìƒ‰ ì†ŒìŠ¤",
        range(len(source_options)),
        format_func=lambda i: source_options[i][0]
    )
    source = source_options[source_selected][1]

# ë©”ì¸ ì½˜í…ì¸ 
tab1, tab2, tab3 = st.tabs(["ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰", "ğŸ“ ìƒˆ í‚¤ì›Œë“œ", "ğŸŒ ì „ì²´ ê²€ìƒ‰"])

with tab1:
    st.subheader("ë“±ë¡ëœ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰")
    keywords = load_keywords()
    
    selected_keyword = st.selectbox("í‚¤ì›Œë“œ ì„ íƒ", keywords, key="keyword_select")
    
    if st.button("ğŸ” ê²€ìƒ‰", key="search_btn1"):
        with st.spinner("ê²€ìƒ‰ ì¤‘..."):
            yahoo_results = [] if source == 'prtimes' else scrape_yahoo_news(selected_keyword, days_ago)
            prtimes_results = [] if source == 'yahoo' else scrape_prtimes(selected_keyword, days_ago)
            all_results = yahoo_results + prtimes_results
        
        st.success(f"'{selected_keyword}' ê²€ìƒ‰ ì™„ë£Œ!")
        st.metric("ì´ ê¸°ì‚¬ ìˆ˜", len(all_results), f"Yahoo: {len(yahoo_results)} | PR Times: {len(prtimes_results)}")
        
        if source == 'both' and (yahoo_results or prtimes_results):
            result_tab1, result_tab2, result_tab3 = st.tabs(["ğŸ“„ ì „ì²´", "ğŸ”” Yahoo", "ğŸ“¢ PR Times"])
            with result_tab1:
                display_articles(all_results)
            with result_tab2:
                display_articles(yahoo_results)
            with result_tab3:
                display_articles(prtimes_results)
        else:
            display_articles(all_results)

with tab2:
    st.subheader("ìƒˆë¡œìš´ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰")
    new_keyword = st.text_input("í‚¤ì›Œë“œ ì…ë ¥", placeholder="ì˜ˆ: AKB48, ä¹ƒæœ¨å‚...")
    
    if st.button("ğŸ” ê²€ìƒ‰", key="search_btn2"):
        if new_keyword:
            with st.spinner("ê²€ìƒ‰ ì¤‘..."):
                yahoo_results = [] if source == 'prtimes' else scrape_yahoo_news(new_keyword, days_ago)
                prtimes_results = [] if source == 'yahoo' else scrape_prtimes(new_keyword, days_ago)
                all_results = yahoo_results + prtimes_results
            
            st.success(f"'{new_keyword}' ê²€ìƒ‰ ì™„ë£Œ!")
            st.metric("ì´ ê¸°ì‚¬ ìˆ˜", len(all_results), f"Yahoo: {len(yahoo_results)} | PR Times: {len(prtimes_results)}")
            
            if source == 'both' and (yahoo_results or prtimes_results):
                result_tab1, result_tab2, result_tab3 = st.tabs(["ğŸ“„ ì „ì²´", "ğŸ”” Yahoo", "ğŸ“¢ PR Times"])
                with result_tab1:
                    display_articles(all_results)
                with result_tab2:
                    display_articles(yahoo_results)
                with result_tab3:
                    display_articles(prtimes_results)
            else:
                display_articles(all_results)
        else:
            st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

with tab3:
    st.subheader("ëª¨ë“  í‚¤ì›Œë“œ ê²€ìƒ‰")
    keywords = load_keywords()
    
    if st.button(f"ğŸ” {len(keywords)}ê°œ í‚¤ì›Œë“œ ëª¨ë‘ ê²€ìƒ‰", key="search_btn3"):
        with st.spinner("ì „ì²´ ê²€ìƒ‰ ì¤‘..."):
            all_keywords_results = {}
            total_count = 0
            
            progress_bar = st.progress(0)
            for idx, keyword in enumerate(keywords):
                yahoo_results = [] if source == 'prtimes' else scrape_yahoo_news(keyword, days_ago)
                prtimes_results = [] if source == 'yahoo' else scrape_prtimes(keyword, days_ago)
                results = yahoo_results + prtimes_results
                
                if results:
                    all_keywords_results[keyword] = results
                    total_count += len(results)
                
                progress_bar.progress((idx + 1) / len(keywords))
        
        st.success(f"ì „ì²´ ê²€ìƒ‰ ì™„ë£Œ!")
        st.metric("ì´ ê¸°ì‚¬ ìˆ˜", total_count)
        
        for keyword in keywords:
            if keyword in all_keywords_results:
                with st.expander(f"**{keyword}** ({len(all_keywords_results[keyword])}ê°œ)"):
                    display_articles(all_keywords_results[keyword])
